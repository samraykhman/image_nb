{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "!playwright install chromium",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Switching to root user to install dependencies...\n",
      "Hit:1 https://download.docker.com/linux/ubuntu jammy InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \n",
      "Hit:3 https://packagecloud.io/github/git-lfs/ubuntu jammy InRelease            \n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease              \n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,375 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/git-core/ppa/ubuntu jammy InRelease     \n",
      "Hit:9 https://ppa.launchpadcontent.net/ondrej/apache2/ubuntu jammy InRelease   \n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,975 kB]\n",
      "Get:12 https://ppa.launchpadcontent.net/ondrej/nginx-mainline/ubuntu jammy InRelease [23.8 kB]\n",
      "Hit:10 https://apt.llvm.org/jammy llvm-toolchain-jammy-17 InRelease\n",
      "Hit:13 https://ppa.launchpadcontent.net/ondrej/php/ubuntu jammy InRelease\n",
      "Get:14 https://ppa.launchpadcontent.net/ondrej/nginx-mainline/ubuntu jammy/main amd64 Packages [9,953 B]\n",
      "Fetched 3,613 kB in 1s (2,642 kB/s)    \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "fonts-freefont-ttf is already the newest version (20120503-10build1).\n",
      "fonts-liberation is already the newest version (1:1.07.4-11).\n",
      "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
      "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
      "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
      "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
      "libcairo-gobject2 is already the newest version (1.16.0-5ubuntu2).\n",
      "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
      "libdbus-glib-1-2 is already the newest version (0.112-2build1).\n",
      "libegl1 is already the newest version (1.4.0-1).\n",
      "libenchant-2-2 is already the newest version (2.3.2-1ubuntu2).\n",
      "libepoxy0 is already the newest version (1.5.10-1).\n",
      "libevdev2 is already the newest version (1.12.1+dfsg-1).\n",
      "libevent-2.1-7 is already the newest version (2.1.12-stable-1build3).\n",
      "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
      "libgles2 is already the newest version (1.4.0-1).\n",
      "libglx0 is already the newest version (1.4.0-1).\n",
      "libgudev-1.0-0 is already the newest version (1:237-2build1).\n",
      "libhyphen0 is already the newest version (2.8.8-7build2).\n",
      "libicu70 is already the newest version (70.1-2).\n",
      "libjpeg-turbo8 is already the newest version (2.1.2-0ubuntu1).\n",
      "liblcms2-2 is already the newest version (2.12~rc1-2build2).\n",
      "libmanette-0.2-0 is already the newest version (0.2.6-3build1).\n",
      "libopengl0 is already the newest version (1.4.0-1).\n",
      "libopenjp2-7 is already the newest version (2.4.0-6).\n",
      "libopus0 is already the newest version (1.3.1-0.1build2).\n",
      "libpng16-16 is already the newest version (1.6.37-3build5).\n",
      "libproxy1v5 is already the newest version (0.4.17-2).\n",
      "libsecret-1-0 is already the newest version (0.20.5-2).\n",
      "libwoff1 is already the newest version (1.0.2-1build4).\n",
      "libxcb-shm0 is already the newest version (1.14-3ubuntu3).\n",
      "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
      "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
      "libxcursor1 is already the newest version (1:1.2.0-2build4).\n",
      "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
      "libxext6 is already the newest version (2:1.3.4-1build1).\n",
      "libxfixes3 is already the newest version (1:6.0.0-1).\n",
      "libxi6 is already the newest version (2:1.8-1build1).\n",
      "libxkbcommon0 is already the newest version (1.4.0-1).\n",
      "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
      "libxrender1 is already the newest version (1:0.9.10-1build4).\n",
      "libxtst6 is already the newest version (2:1.2.3-1build4).\n",
      "xfonts-scalable is already the newest version (1:1.0.3-1.2ubuntu1).\n",
      "fonts-ipafont-gothic is already the newest version (00303-21ubuntu1).\n",
      "fonts-tlwg-loma-otf is already the newest version (1:0.7.3-1).\n",
      "fonts-unifont is already the newest version (1:14.0.01-1).\n",
      "fonts-wqy-zenhei is already the newest version (0.9.45-8).\n",
      "libffi7 is already the newest version (3.3-5ubuntu1).\n",
      "libx264-163 is already the newest version (2:0.163.3060+git5db6aa6-2build1).\n",
      "xfonts-cyrillic is already the newest version (1:1.0.5).\n",
      "fonts-noto-color-emoji is already the newest version (2.042-0ubuntu0.22.04.1).\n",
      "gstreamer1.0-plugins-base is already the newest version (1.20.1-1ubuntu0.1).\n",
      "gstreamer1.0-plugins-good is already the newest version (1.20.3-0ubuntu1.1).\n",
      "libatomic1 is already the newest version (12.3.0-1ubuntu1~22.04).\n",
      "libcups2 is already the newest version (2.4.1op1-1ubuntu4.8).\n",
      "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
      "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
      "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
      "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.2).\n",
      "libgdk-pixbuf-2.0-0 is already the newest version (2.42.8+dfsg-1ubuntu0.2).\n",
      "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.2).\n",
      "libgstreamer-gl1.0-0 is already the newest version (1.20.1-1ubuntu0.1).\n",
      "libgstreamer-plugins-base1.0-0 is already the newest version (1.20.1-1ubuntu0.1).\n",
      "libgstreamer1.0-0 is already the newest version (1.20.3-0ubuntu1).\n",
      "libgtk-3-0 is already the newest version (3.24.33-1ubuntu2).\n",
      "libharfbuzz-icu0 is already the newest version (2.7.4-1ubuntu3.1).\n",
      "libharfbuzz0b is already the newest version (2.7.4-1ubuntu3.1).\n",
      "libnotify4 is already the newest version (0.7.9-3ubuntu5.22.04.1).\n",
      "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
      "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
      "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
      "libpangocairo-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
      "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
      "libwayland-egl1 is already the newest version (1.20.0-1ubuntu0.1).\n",
      "libwayland-server0 is already the newest version (1.20.0-1ubuntu0.1).\n",
      "libwebpdemux2 is already the newest version (1.2.2-2ubuntu0.22.04.2).\n",
      "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
      "libx11-xcb1 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
      "libxslt1.1 is already the newest version (1.1.34-4ubuntu0.22.04.1).\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "gstreamer1.0-libav is already the newest version (1.20.3-0ubuntu1).\n",
      "gstreamer1.0-plugins-bad is already the newest version (1.20.3-0ubuntu1.1).\n",
      "libsoup-3.0-0 is already the newest version (3.0.7-0ubuntu1).\n",
      "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.10).\n",
      "libxml2 is already the newest version (2.9.14+dfsg-0.1+ubuntu22.04.1+deb.sury.org+1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": "!playwright install-deps"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T16:45:24.132223Z",
     "start_time": "2024-04-26T16:45:22.257096Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install requests",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests) (1.26.5)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2020.6.20)\r\n",
      "\u001B[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: python-debian 0.1.43ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T16:46:08.549147Z",
     "start_time": "2024-04-26T16:46:06.486730Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install bs4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\r\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\r\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\r\n",
      "\u001B[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: python-debian 0.1.43ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001B[0m\u001B[33m\r\n",
      "\u001B[0mInstalling collected packages: bs4\r\n",
      "Successfully installed bs4-0.0.2\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T16:52:25.860866Z",
     "start_time": "2024-04-26T16:52:25.739946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import requests\n",
    "from datetime import date\n",
    "from gzip import decompress\n",
    "import asyncio\n",
    "import os\n",
    "# from playwright.async_api import async_playwright, Browser, Playwright, Page, APIResponseAssertions\n",
    "from bs4 import BeautifulSoup\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T17:05:42.463662Z",
     "start_time": "2024-04-26T17:05:42.457179Z"
    }
   },
   "cell_type": "code",
   "source": "print('fuck you world')\n",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck you world\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# bs_xml = BeautifulSoup(bs_data, 'xml')\n",
    "\n",
    "# # need to figure out which tag we're looking for\n",
    "\n",
    "# # finds all instance of the tag\n",
    "# b_unique = bs_xml.find_all('loc')\n",
    "\n",
    "# #extracts attribute of the first instance of the tag\n",
    "# b_name = bs_xml.find('child', {'name': 'grid_block?'})\n",
    "\n",
    "# # extracting data stored in a specific attribute of the child\n",
    "# data_info = b_name.get('data-tc-analytics')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadGzipFile",
     "evalue": "Not a gzipped file (b'<H')",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBadGzipFile\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m/workspace/image_nb/farfetch_xml_parser.ipynb Cell 7\u001B[0m line \u001B[0;36m2\n\u001B[1;32m      <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001B[0m r \u001B[39m=\u001B[39m requests\u001B[39m.\u001B[39mget(\u001B[39m'\u001B[39m\u001B[39mhttps://www.farfetch.com/sitemaps/sitemaps-farfetch-com/us-sitemap-products-1.xml.gz\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[0;32m----> <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001B[0m bs_xml \u001B[39m=\u001B[39m BeautifulSoup(decompress(r\u001B[39m.\u001B[39;49mcontent), \u001B[39m'\u001B[39m\u001B[39mxml\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m      <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001B[0m \u001B[39m# product_list = []\u001B[39;00m\n\u001B[1;32m      <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001B[0m loc_elements \u001B[39m=\u001B[39m bs_xml\u001B[39m.\u001B[39mfind_all(\u001B[39m'\u001B[39m\u001B[39mloc\u001B[39m\u001B[39m'\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/python3.10/gzip.py:557\u001B[0m, in \u001B[0;36mdecompress\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    553\u001B[0m \u001B[39m\u001B[39m\u001B[39m\"\"\"Decompress a gzip compressed string in one shot.\u001B[39;00m\n\u001B[1;32m    554\u001B[0m \u001B[39mReturn the decompressed string.\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    556\u001B[0m \u001B[39mwith\u001B[39;00m GzipFile(fileobj\u001B[39m=\u001B[39mio\u001B[39m.\u001B[39mBytesIO(data)) \u001B[39mas\u001B[39;00m f:\n\u001B[0;32m--> 557\u001B[0m     \u001B[39mreturn\u001B[39;00m f\u001B[39m.\u001B[39;49mread()\n",
      "File \u001B[0;32m/usr/lib/python3.10/gzip.py:301\u001B[0m, in \u001B[0;36mGzipFile.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    299\u001B[0m     \u001B[39mimport\u001B[39;00m \u001B[39merrno\u001B[39;00m\n\u001B[1;32m    300\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mOSError\u001B[39;00m(errno\u001B[39m.\u001B[39mEBADF, \u001B[39m\"\u001B[39m\u001B[39mread() on write-only GzipFile object\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m--> 301\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_buffer\u001B[39m.\u001B[39;49mread(size)\n",
      "File \u001B[0;32m/usr/lib/python3.10/_compression.py:118\u001B[0m, in \u001B[0;36mDecompressReader.readall\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    114\u001B[0m chunks \u001B[39m=\u001B[39m []\n\u001B[1;32m    115\u001B[0m \u001B[39m# sys.maxsize means the max length of output buffer is unlimited,\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[39m# so that the whole input buffer can be decompressed within one\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[39m# .decompress() call.\u001B[39;00m\n\u001B[0;32m--> 118\u001B[0m \u001B[39mwhile\u001B[39;00m data \u001B[39m:=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mread(sys\u001B[39m.\u001B[39;49mmaxsize):\n\u001B[1;32m    119\u001B[0m     chunks\u001B[39m.\u001B[39mappend(data)\n\u001B[1;32m    121\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mb\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m.\u001B[39mjoin(chunks)\n",
      "File \u001B[0;32m/usr/lib/python3.10/gzip.py:488\u001B[0m, in \u001B[0;36m_GzipReader.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    484\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_new_member:\n\u001B[1;32m    485\u001B[0m     \u001B[39m# If the _new_member flag is set, we have to\u001B[39;00m\n\u001B[1;32m    486\u001B[0m     \u001B[39m# jump to the next member, if there is one.\u001B[39;00m\n\u001B[1;32m    487\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_init_read()\n\u001B[0;32m--> 488\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_read_gzip_header():\n\u001B[1;32m    489\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_size \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_pos\n\u001B[1;32m    490\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mb\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m\"\u001B[39m\n",
      "File \u001B[0;32m/usr/lib/python3.10/gzip.py:436\u001B[0m, in \u001B[0;36m_GzipReader._read_gzip_header\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    433\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mFalse\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \u001B[39mif\u001B[39;00m magic \u001B[39m!=\u001B[39m \u001B[39mb\u001B[39m\u001B[39m'\u001B[39m\u001B[39m\\037\u001B[39;00m\u001B[39m\\213\u001B[39;00m\u001B[39m'\u001B[39m:\n\u001B[0;32m--> 436\u001B[0m     \u001B[39mraise\u001B[39;00m BadGzipFile(\u001B[39m'\u001B[39m\u001B[39mNot a gzipped file (\u001B[39m\u001B[39m%r\u001B[39;00m\u001B[39m)\u001B[39m\u001B[39m'\u001B[39m \u001B[39m%\u001B[39m magic)\n\u001B[1;32m    438\u001B[0m (method, flag,\n\u001B[1;32m    439\u001B[0m  \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_last_mtime) \u001B[39m=\u001B[39m struct\u001B[39m.\u001B[39munpack(\u001B[39m\"\u001B[39m\u001B[39m<BBIxx\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_read_exact(\u001B[39m8\u001B[39m))\n\u001B[1;32m    440\u001B[0m \u001B[39mif\u001B[39;00m method \u001B[39m!=\u001B[39m \u001B[39m8\u001B[39m:\n",
      "\u001B[0;31mBadGzipFile\u001B[0m: Not a gzipped file (b'<H')"
     ]
    }
   ],
   "source": [
    "# r = requests.get('https://www.farfetch.com/sitemaps/sitemaps-farfetch-com/us-sitemap-products-1.xml.gz')\n",
    "# bs_xml = BeautifulSoup(decompress(r.content), 'xml')\n",
    "\n",
    "# # product_list = []\n",
    "\n",
    "# loc_elements = bs_xml.find_all('loc')\n",
    "# print(loc_elements[0])\n",
    "# print(len(loc_elements))\n",
    "# # for i in range(1:len(xmlfile)):\n",
    "# #     b_unique = bs_xml.find_all('loc')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T16:47:25.557412Z",
     "start_time": "2024-04-26T16:47:25.552239Z"
    }
   },
   "source": [
    "def get_public_ip():\n",
    "    params = {\n",
    "    'format': 'json',\n",
    "}\n",
    "\n",
    "    response = requests.get('https://api.ipify.org', params=params)\n",
    "    return response.body"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T16:47:28.040890Z",
     "start_time": "2024-04-26T16:47:27.702595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'}\n",
    "url = 'https://www.farfetch.com/sitemaps/sitemaps-farfetch-com/us-sitemap-products-1.xml.gz'\n",
    "r = requests.get(url, headers=headers)\n",
    "\n",
    "print(get_public_ip())\n",
    "if r.status_code == 200:\n",
    "    # Decompress the gzip content\n",
    "    content = decompress(r.content)\n",
    "\n",
    "    # Parse the XML content with BeautifulSoup\n",
    "    bs_xml = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "    # Find all 'loc' elements\n",
    "    loc_elements = bs_xml.find_all('loc')\n",
    "\n",
    "    # Optional: Print the first 'loc' element and the total number of 'loc' elements\n",
    "    if loc_elements:\n",
    "        print(loc_elements[0].text)\n",
    "    print(len(loc_elements))\n",
    "else:\n",
    "    print(\"Failed to retrieve the data\")\n",
    "    print(r.status_code)\n"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Response' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.farfetch.com/sitemaps/sitemaps-farfetch-com/us-sitemap-products-1.xml.gz\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m r \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(url, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mget_public_ip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# Decompress the gzip content\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     content \u001B[38;5;241m=\u001B[39m decompress(r\u001B[38;5;241m.\u001B[39mcontent)\n",
      "Cell \u001B[0;32mIn[5], line 7\u001B[0m, in \u001B[0;36mget_public_ip\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m     params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      4\u001B[0m }\n\u001B[1;32m      6\u001B[0m     response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://api.ipify.org\u001B[39m\u001B[38;5;124m'\u001B[39m, params\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m----> 7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Response' object has no attribute 'body'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.farfetch.com/shopping/women/adidas-falcon-low-top-sneakers-item-15946776.aspx\n",
      "{\"ip\":\"34.145.59.229\"}\n",
      "429\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m/workspace/image_nb/farfetch_xml_parser.ipynb Cell 7\u001B[0m line \u001B[0;36m1\n\u001B[1;32m     <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001B[0m \u001B[39m# s = soup.find('div', class_ = 'ltr-1vr8bhw')\u001B[39;00m\n\u001B[1;32m     <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001B[0m s \u001B[39m=\u001B[39m soup\u001B[39m.\u001B[39mfind(\u001B[39m'\u001B[39m\u001B[39mdiv\u001B[39m\u001B[39m'\u001B[39m, class_ \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mltr-ukwwcv\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[0;32m---> <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001B[0m s2 \u001B[39m=\u001B[39m s\u001B[39m.\u001B[39;49mfind(\u001B[39m'\u001B[39m\u001B[39mdiv\u001B[39m\u001B[39m'\u001B[39m, class_ \u001B[39m=\u001B[39m \u001B[39m'\u001B[39m\u001B[39mltr-1vr8bhw\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m     <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001B[0m \u001B[39mif\u001B[39;00m s2 \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m     <a href='vscode-notebook-cell://samraykhman-imagenb-ofo5eja97sd.ws-us110.gitpod.io/workspace/image_nb/farfetch_xml_parser.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001B[0m     lines \u001B[39m=\u001B[39m s2\u001B[39m.\u001B[39mfind_all(tags)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "execution_count": 92,
   "source": [
    "small_loc_elements = loc_elements[3]\n",
    "\n",
    "tags = ['a','p','li']\n",
    "\n",
    "for element in small_loc_elements:\n",
    "    print(element)\n",
    "    print(get_public_ip())\n",
    "    r = requests.get(element.text, headers=headers)\n",
    "    print(r.status_code)\n",
    "    variable = r.content\n",
    "    \n",
    "    html_string = variable.decode('utf-8')\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "    # s = soup.find('div', class_ = 'ltr-1vr8bhw')\n",
    "    s = soup.find('div', class_ = 'ltr-ukwwcv')\n",
    "    s2 = s.find('div', class_ = 'ltr-1vr8bhw')\n",
    "    \n",
    "    if s2 is not None:\n",
    "        lines = s2.find_all(tags)\n",
    "        for index, line in enumerate(lines):\n",
    "            print(str(index) + line.text)\n",
    "    else:\n",
    "        print(\"Div not found for URL:\", element.text)\n",
    "    \n",
    "    # lines = s.find_all(tags)\n",
    "    # for line in lines:\n",
    "    #     print(line.text)\n",
    "    \n",
    "    \n",
    "    # print(html_string)\n",
    "    # fullname = 'ooogabooga.txt'\n",
    "    # open(f'{fullname}','w',encoding='utf-8').write(html_string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# small_loc_elements = loc_elements[1]  # Assuming loc_elements is already defined\n",
    "# tag_contents = {tag: [] for tag in tags}  # Dictionary to store text by tags\n",
    "\n",
    "\n",
    "\n",
    "def tag_content_creator(tags, elements_list):\n",
    "    # tag_contents = {tag: [] for tag in tags}  # Dictionary to store text by tags\n",
    "    # cwd = os.getcwd()\n",
    "    for element in elements_list:\n",
    "        r = requests.get(element.text, headers=headers)\n",
    "        print(r.status_code)\n",
    "        response = r.content  # Fetch the page content\n",
    "        html_string = response.decode('utf-8')  # Decode the content to string\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')  # Parse HTML\n",
    "        # print(html_string.status_code)\n",
    "        target_div = soup.find('div', class_='ltr-ukwwcv')  # Find the specific div\n",
    "        target_div_child = target_div.find('div', class_='ltr-1vr8bhw')\n",
    "        # print(response.status_code)\n",
    "\n",
    "        if target_div_child:  # Check if the target div is found\n",
    "            for tag in tags:\n",
    "                lines = target_div_child.find_all(tag)  # Find all elements for each tag\n",
    "                for line in lines:\n",
    "                    with open('farfetch_info.tsv','w', encoding='utf-8') as f:\n",
    "                        if 'Sorry' in line.text:\n",
    "                            f.write(element + '\\t' + 'Sold Out' + '\\n')\n",
    "                        else:\n",
    "                            f.write(element + '\\t' + line.text + '\\n')\n",
    "    \n",
    "    return\n",
    "    #                 tag_contents[tag].append(line.text)  # Append the text to the respective tag list\n",
    "    # return tag_contents\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tags = ['a', 'p', 'li']\n",
    "\n",
    "print(tag_content_creator(tags, small_loc_elements))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_loc_elements = loc_elements[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<loc>https://www.farfetch.com/shopping/women/khaite-keese-ribbed-knit-dress-item-19417826.aspx</loc>]\n"
     ]
    }
   ],
   "source": [
    "print(small_loc_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Example HTML content in bytes\n",
    "html_bytes = b\"<html><head><title>Example Page</title></head><body><p>Hello, world!</p></body></html>\"\n",
    "\n",
    "# Decode the bytes to a string, assuming the encoding is UTF-8\n",
    "html_string = html_bytes.decode('utf-8')\n",
    "\n",
    "# Parse the string using BeautifulSoup\n",
    "soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "# Now you can work with `soup` to find elements, navigate the tree, etc.\n",
    "print(soup.title.text)  # Output: Example Page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to https://www.farfetch.com/sitemaps/sitemaps-farfetch-com/us-sitemap-products-1.xml.gz\n",
      "Received response with status code: 200\n",
      "Decompressing the content\n",
      "Decompressed content length: 34334253\n",
      "Parsing content with BeautifulSoup\n",
      "Finding all 'loc' elements\n",
      "First 'loc' element: https://www.farfetch.com/shopping/women/patek-philippe-2005-pre-owned-calatrava-25mm-item-22486291.aspx\n",
      "Total number of 'loc' elements found: 50000\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import gzip\n",
    "# from io import BytesIO\n",
    "\n",
    "# # Send a GET request to the URL\n",
    "# url = 'https://www.farfetch.com/sitemaps/sitemaps-farfetch-com/us-sitemap-products-1.xml.gz'\n",
    "# print(f\"Sending request to {url}\")\n",
    "\n",
    "# headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'}\n",
    "# r = requests.get(url, headers=headers)\n",
    "# print(f\"Received response with status code: {r.status_code}\")\n",
    "\n",
    "# # Check if the response was successful\n",
    "# if r.status_code == 200:\n",
    "#     print(\"Decompressing the content\")\n",
    "#     # Decompress the gzip content\n",
    "#     content = gzip.decompress(r.content)\n",
    "#     print(f\"Decompressed content length: {len(content)}\")\n",
    "\n",
    "#     # Parse the XML content with BeautifulSoup\n",
    "#     print(\"Parsing content with BeautifulSoup\")\n",
    "#     bs_xml = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "#     # Find all 'loc' elements\n",
    "#     print(\"Finding all 'loc' elements\")\n",
    "#     loc_elements = bs_xml.find_all('loc')\n",
    "    \n",
    "#     # Print the first 'loc' element, if it exists\n",
    "#     if loc_elements:\n",
    "#         print(f\"First 'loc' element: {loc_elements[0].text}\")\n",
    "#     else:\n",
    "#         print(\"No 'loc' elements found\")\n",
    "    \n",
    "#     # Print the total number of 'loc' elements found\n",
    "#     print(f\"Total number of 'loc' elements found: {len(loc_elements)}\")\n",
    "# else:\n",
    "#     print(\"Failed to retrieve the data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [], 'p': [], 'li': []}\n"
     ]
    }
   ],
   "source": [
    "tag_contents = {tag: [] for tag in tags}  # Dictionary to store text by tags\n",
    "for element in small_loc_elements:\n",
    "    response = requests.get(element.text, headers=headers)  # Fetch the page content\n",
    "    html_string = response.content.decode('utf-8')  # Decode the content to string\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')  # Parse HTML\n",
    "    target_div = soup.find('div', class_='ltr-1vr8bhw')  # Find the specific div\n",
    "\n",
    "    if target_div:  # Check if the target div is found\n",
    "        for tag in tags:\n",
    "            lines = target_div.find_all(tag)  # Find all elements for each tag\n",
    "            for line in lines:\n",
    "                tag_contents[tag].append(line.text)  # Append the text to the respective tag list\n",
    "\n",
    "print(tag_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'a': [], 'p': [], 'li': []}}\n"
     ]
    }
   ],
   "source": [
    "tags = ['a', 'p', 'li']\n",
    "\n",
    "element_range = [i for i in range(len(small_loc_elements))]\n",
    "\n",
    "farf_link_dict = {n: tag_content_creator(tags, small_loc_elements) for n in element_range}\n",
    "\n",
    "print(farf_link_dict)\n",
    "\n",
    "\n",
    "# for i in range(len(small_loc_elements)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mattermost.com/blog/how-to-scrape-website-data-using-python/\n",
    "\n",
    "Python Tutorial: Web Scraping with Requests-HTML --> https://www.youtube.com/watch?v=a6fIbtFB46g\n",
    "\n",
    "Python Requests Tutorial: Request Web Pages, Download Images, POST Data, Read JSON, and More --> https://www.youtube.com/watch?v=tb8gHvYlCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_dict(number):\n",
    "    return {'square': number ** 2, 'cube': number ** 3}\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]  # This is the list of numbers\n",
    "\n",
    "# Using dictionary comprehension with a function\n",
    "result_dict = {n: create_value_dict(n) for n in numbers}\n",
    "\n",
    "print(result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
